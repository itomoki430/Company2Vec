{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys \n",
    "sys.path.append(\"./src\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from argparse import ArgumentParser\n",
    "from pathlib import Path\n",
    "import os\n",
    "import torch\n",
    "import logging\n",
    "import json\n",
    "import random\n",
    "import numpy as np\n",
    "from collections import namedtuple\n",
    "from tempfile import TemporaryDirectory\n",
    "\n",
    "from torch.utils.data import DataLoader, Dataset, RandomSampler\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "from tqdm import tqdm\n",
    "\n",
    "from pytorch_transformers import WEIGHTS_NAME, CONFIG_NAME\n",
    "from pytorch_transformers.modeling_bert import BertForPreTraining, BertForSequenceClassification\n",
    "from pytorch_transformers.tokenization_bert import BertTokenizer\n",
    "from pytorch_transformers.optimization import AdamW, WarmupLinearSchedule\n",
    "from pathlib import Path\n",
    "import re\n",
    "\n",
    "import time\n",
    "import pandas as pd\n",
    "from sklearn.metrics import classification_report\n",
    "import  matplotlib.pyplot as plt\n",
    "import scipy\n",
    "\n",
    "from pytorch_transformers import BertPreTrainedModel, BertModel, BertForSequenceClassification, BertConfig\n",
    "import json\n",
    "import math\n",
    "import os\n",
    "import sys\n",
    "from io import open\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import CrossEntropyLoss, MSELoss\n",
    "import gzip\n",
    "logger = logging.getLogger(__name__)\n",
    "import  matplotlib.pyplot as plt\n",
    "\n",
    "from collections import defaultdict\n",
    "import sklearn\n",
    "from torch import functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import glob\n",
    "from multiprocessing import Pool\n",
    "import time\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import pickle\n",
    "from multiprocessing import Pool, cpu_count\n",
    "import requests\n",
    "import copy\n",
    "import ml_metrics "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "read_file_name = \"models/20200120_BERTMeanVEC/OnlyStock/pytorch_model.bin\"\n",
    "\n",
    "# use all labels\n",
    "read_file_name = \"models/20200120_BERTMeanVEC/Sector_1/01/pytorch_model.bin\"\n",
    "read_file_name = \"models/20200120_BERTMeanVEC/Stock_Sector_1/01/pytorch_model.bin\"\n",
    "read_file_name = \"models/20200120_BERTMeanVEC/Sector_SectorName_1/01_2/pytorch_model.bin\"\n",
    "read_file_name = \"models/20200120_BERTMeanVEC/Stock_Sector_SectorName_1/01/pytorch_model.bin\"\n",
    "\n",
    "# use two  labels\n",
    "read_file_name = \"models/20200120_BERTMeanVEC/Sector_Name_2labels/01/pytorch_model.bin\"\n",
    "read_file_name = \"models/20200120_BERTMeanVEC/Stock_Sector_Name_2labels/01/pytorch_model.bin\"\n",
    "# use  five labels\n",
    "read_file_name = \"models/20200120_BERTMeanVEC/Sector_Name_5labels/01/pytorch_model.bin\"\n",
    "read_file_name = \"models/20200120_BERTMeanVEC/Stock_Sector_Name_5labels/01/pytorch_model.bin\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_seq_length = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "return_val_dict = pickle.load(open(\"data/return_val_dict.pkl\", \"rb\"))\n",
    "df_ticker = pd.read_csv(\"data/ticker_list.csv\")\n",
    "text_data_id_df = pd.read_csv(\"data/text_data_id.csv\")\n",
    "month_stock_info_df = pd.read_csv(\"data/stock_price.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "ticker2sector = dict(zip(df_ticker[\"ticker\"], df_ticker[\"sector\"]))\n",
    "ticker2industry = dict(zip(df_ticker[\"ticker\"], df_ticker[\"industry\"]))\n",
    "all_month_list = np.sort(list(set(month_stock_info_df[\"month\"])))\n",
    "month2index = dict(zip(all_month_list, range(len(all_month_list))))\n",
    "text_data = dict(zip(text_data_id_df[\"ticker\"], [\n",
    "    [np.array(text_data_id_df[\"text_id\"].iloc[i][1:-1].split(\",\")).astype(np.int32)] for i in range(len(text_data_id_df))]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_stock_price_list = []\n",
    "sector_array = []\n",
    "industry_array = []\n",
    "use_text_data = []\n",
    "use_ticker_list_data = []\n",
    "stock_size = 59\n",
    "for index, ticker in enumerate(df_ticker[\"ticker\"]):\n",
    "    if ticker in return_val_dict:\n",
    "        if len(return_val_dict[ticker]) >= 12:\n",
    "            if  (type(ticker2sector[ticker]) == str) :\n",
    "                use_stock_price_list.append(return_val_dict[ticker][-stock_size:])\n",
    "                sector_array.append(ticker2sector[ticker])\n",
    "                use_text_data.append(text_data[ticker])\n",
    "                industry_array.append(ticker2industry[ticker])\n",
    "                use_ticker_list_data.append(ticker)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity_mat_df = pd.read_csv(\"../AnalysisEnglishAnnualReports/BertIndustryClustering/stock_similarity_mat/all_similarity_mat.csv\", \n",
    "                                index_col=0)\n",
    "similarity_mat_df = similarity_mat_df[df_ticker[\"ticker\"]].T[df_ticker[\"ticker\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity_inner_mat = np.array(similarity_mat_df[\n",
    "    [len(return_val_dict[ticker]) > 12 for index, ticker in enumerate(df_ticker[\"ticker\"])]].T[\n",
    "    [len(return_val_dict[ticker]) > 12 for index, ticker in enumerate(df_ticker[\"ticker\"])]\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "sector2id = dict(zip(np.sort(list(set(sector_array))), range(len(set(sector_array)))))\n",
    "industry2id = dict(zip(np.sort(list(set(industry_array))), range(len(set(industry_array)))))\n",
    "use_label_data = [sector2id[sector] for sector in sector_array]\n",
    "use_industry_data = [industry2id[sector] for sector in industry_array]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict Using Bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from NN import *\n",
    "from evaluation import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "gradient_accumulation_steps = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2462"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(use_label_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_size = 1800\n",
    "valid_data_size = 262\n",
    "test_data_size = (len(use_label_data) - (train_data_size + valid_data_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_x = use_text_data[0:train_data_size]\n",
    "train_data_y = use_label_data[0:train_data_size]\n",
    "train_data_industry = use_industry_data[0:train_data_size]\n",
    "train_inner_stock_mat = similarity_inner_mat[:train_data_size, :train_data_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_data_x = use_text_data[train_data_size:train_data_size + valid_data_size]\n",
    "valid_data_y = use_label_data[train_data_size:train_data_size + valid_data_size]\n",
    "valid_data_industry = use_industry_data[train_data_size:train_data_size + valid_data_size]\n",
    "valid_inner_stock_mat = similarity_inner_mat[train_data_size:train_data_size + valid_data_size, \n",
    "                                                                                  train_data_size:train_data_size + valid_data_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_x = use_text_data[-test_data_size:]\n",
    "test_data_y = use_label_data[-test_data_size:]\n",
    "test_data_industry = use_industry_data[-test_data_size:]\n",
    "test_inner_stock_mat = similarity_inner_mat[-test_data_size:, -test_data_size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_stock_label_y = np.array(use_stock_price_list)[0:train_data_size]\n",
    "valid_label_y = np.array(use_stock_price_list)[train_data_size:train_data_size + valid_data_size]\n",
    "test_stock_label_y = np.array(use_stock_price_list)[-test_data_size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((400,), (1800,))"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_stock_label_y.shape, train_stock_label_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_size =    1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_train_examples = len(train_data_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_num = max(use_label_data)+ 1\n",
    "industry_num = max(use_industry_data)+ 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[01;34m001\u001b[0m/  \u001b[01;34m01\u001b[0m/  \u001b[01;34m01_1\u001b[0m/  \u001b[01;34m1\u001b[0m/\r\n"
     ]
    }
   ],
   "source": [
    "ls ../AnalysisEnglishAnnualReports/BertIndustryClustering/20200120_BERTMeanVEC/Stock_Sector_SectorName_1/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "def out_actual_predicted_list(sentence_representation_all, theme_word_list, pooled_output, th_val = 0):\n",
    "    actual_list = []\n",
    "    predicted_list = []\n",
    "    for word, theme_vector_gpu in zip(theme_word_list, pooled_output):\n",
    "        theme_vector = theme_vector_gpu.detach().to(\"cpu\").numpy()\n",
    "        if np.mean(np.array(test_data_industry) == industry2id[word])  <= th_val:\n",
    "            continue\n",
    "        #print (word)\n",
    "        doc2theme_similarity_mat = sklearn.metrics.pairwise.cosine_similarity(sentence_representation_all, theme_vector.reshape(1, -1))\n",
    "        actual = np.array(range(len(test_data_industry)))[np.array(test_data_industry) == industry2id[word]]\n",
    "        predicted = np.argsort(doc2theme_similarity_mat[:,0])[-1::-1]\n",
    "        actual_list.append(list(actual))\n",
    "        predicted_list.append(list(predicted))\n",
    "    return actual_list, predicted_list "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_vectors(folda_name, use_ticker_list_train, sentence_representation_all_train, \n",
    "                use_ticker_list_valid, sentence_representation_all_valid, \n",
    "                use_ticker_list_test, sentence_representation_all_test):\n",
    "    with open(folda_name + \"/train_ticker_vectors.txt\", \"w\") as f:\n",
    "        for ticker, vector in zip(use_ticker_list_train, sentence_representation_all_train):\n",
    "            f.write(\"\\t\".join([ticker] + [str(num) for num in vector]) + \"\\n\")\n",
    "    \n",
    "    with open(folda_name + \"/valid_ticker_vectors.txt\", \"w\") as f:\n",
    "        for ticker, vector in zip(use_ticker_list_valid, sentence_representation_all_valid):\n",
    "            f.write(\"\\t\".join([ticker] + [str(num) for num in vector]) + \"\\n\")\n",
    "    \n",
    "    with open(folda_name + \"/test_ticker_vectors.txt\", \"w\") as f:\n",
    "        for ticker, vector in zip(use_ticker_list_test, sentence_representation_all_test):\n",
    "            f.write(\"\\t\".join([ticker] + [str(num) for num in vector]) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_config = BertConfig.from_pretrained('models/PreTrainigNLMbyBert/finetuned_lm/')\n",
    "#model_state_dict = torch.load(read_file_name + \"pytorch_model.bin\")\n",
    "model_state_dict = torch.load(read_file_name)\n",
    "model = BertForSequenceMeanVec(bert_config, label_num, industry_num, state_dict=model_state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_size =   1\n",
    "batch_size = 16\n",
    "test_data_size = len(test_data_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fct = CrossEntropyLoss(ignore_index = -1)\n",
    "loss_mse = MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sector\n",
      "same:  0.6118864\n",
      "other:  0.3555874\n",
      "industry\n",
      "same:  0.75139105\n",
      "other:  0.38757405\n"
     ]
    }
   ],
   "source": [
    "sentence_representation_all_test = evaluate_model_with_cosine_similarity(model, test_data_x, test_data_y, test_data_industry, test_data_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc2soc_similarity_mat = sklearn.metrics.pairwise.cosine_similarity(sentence_representation_all_test)\n",
    "sort_values = np.argsort(doc2soc_similarity_mat, axis = 1)[:,-1::-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MAP@K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sector (MAP@K)\n",
      "5 : 0.5308083333333333\n",
      "10 : 0.4870513888888888\n",
      "50 : 0.3793404444751927\n",
      "industry (MAP@K)\n",
      "5 : 0.31914791666666664\n",
      "10 : 0.3274141424162258\n",
      "50 : 0.34913729978394925\n"
     ]
    }
   ],
   "source": [
    "print (\"sector (MAP@K)\")\n",
    "predicted_list = [list(item[1:]) for item in sort_values]\n",
    "actual_list_all= [list(np.array(range(len(test_data_y)))[np.array(test_data_y) == test_data_y[index]]) for index in range(len(test_data_y))]\n",
    "actual_list_rev = []\n",
    "for index, item in enumerate(actual_list_all):\n",
    "    actual_list_rev.append(list(np.array(item)[np.array(item) != index]))\n",
    "for top_n in [5,10, 50]:\n",
    "    print (top_n , \":\",  ml_metrics.mapk(actual_list_rev, predicted_list, top_n))\n",
    "\n",
    "print (\"industry (MAP@K)\")\n",
    "predicted_list = [list(item[1:]) for item in sort_values]\n",
    "actual_list_all= [list(np.array(range(len(test_data_industry)))[np.array(test_data_industry) == test_data_industry[index]]) for index in range(len(test_data_industry))]\n",
    "actual_list_rev = []\n",
    "for index, item in enumerate(actual_list_all):\n",
    "    actual_list_rev.append(list(np.array(item)[np.array(item) != index]))\n",
    "for top_n in [5,10, 50]:\n",
    "    print (top_n , \":\",  ml_metrics.mapk(actual_list_rev, predicted_list, top_n))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Thene Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "theme_word_list = [word for word in industry2id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [],
   "source": [
    "theme_token_list = []\n",
    "for word in theme_word_list:\n",
    "    text = \"[CLS] \" + word +  \" [SEP]\"\n",
    "    #text = word\n",
    "    tokenized_text = tokenizer.tokenize(text)\n",
    "    tokenized_id = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
    "    theme_token_list.append(tokenized_id)\n",
    "\n",
    "max_seq_length_company = 8\n",
    "input_ids_list = []\n",
    "for tokenized_id in theme_token_list:\n",
    "    input_array = np.zeros(max_seq_length_company, dtype=np.int)\n",
    "    input_array[:min(max_seq_length_company, len(tokenized_id))] = tokenized_id[:min(max_seq_length_company, len(tokenized_id))]\n",
    "    input_ids_list.append(input_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = torch.LongTensor(np.array(input_ids_list).astype(np.int32))\n",
    "label_logits, pooled_output = model(input_ids, labels= None,  \n",
    "                                                label_industry = None,\n",
    "                                                labels_stock =  None, stock_vol = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "StockTheme\n",
      "140\n",
      "5 : 0.23416865079365076\n",
      "10 : 0.25440026724975706\n",
      "50 : 0.27902750448714714\n"
     ]
    }
   ],
   "source": [
    "actual_list, predicted_list  = out_actual_predicted_list(sentence_representation_all_test, theme_word_list, pooled_output, 0)\n",
    "print (\"StockTheme\")\n",
    "print (len(actual_list))\n",
    "for k in [5,10,50]:\n",
    "        print (k, \":\", ml_metrics.mapk(actual_list, predicted_list, k))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
