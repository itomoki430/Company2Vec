{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys \n",
    "sys.path.append(\"./src\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import logging\n",
    "import os\n",
    "import torch\n",
    "import MeCab\n",
    "from pytorch_pretrained_bert import BertTokenizer, BertModel, BertForMaskedLM, WordpieceTokenizer\n",
    "from pytorch_pretrained_bert.tokenization import load_vocab\n",
    "from pytorch_transformers import BertPreTrainedModel, BertModel, BertForSequenceClassification, BertConfig\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import CrossEntropyLoss, MSELoss\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pytorch_transformers.optimization import AdamW, WarmupLinearSchedule\n",
    "from collections import defaultdict\n",
    "import sklearn\n",
    "import sklearn.metrics\n",
    "import  matplotlib.pyplot as plt\n",
    "import time\n",
    "from sklearn.metrics import classification_report\n",
    "import copy\n",
    "from multiprocessing import Pool, cpu_count\n",
    "import requests\n",
    "import ml_metrics \n",
    "from torch import functional as F\n",
    "import sys\n",
    "import datetime\n",
    "from dateutil.relativedelta import relativedelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bert_ja import *\n",
    "from evaluation import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:pytorch_pretrained_bert.tokenization:loading vocabulary file ./models/PyTorchPretrainendModel/vocab.txt\n"
     ]
    }
   ],
   "source": [
    "tokenizer = BertMeCabTokenizer.from_pretrained('./models/PyTorchPretrainendModel/vocab.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_list = list(tokenizer.vocab.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_data_size = 2200\n",
    "valid_size = 316\n",
    "test_size = 500\n",
    "gradient_accumulation_steps = 1\n",
    "max_epoch = 5\n",
    "max_seq_length = 512\n",
    "batch_size = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data = pd.read_csv(\"dataset/ticker_list.csv\", index_col=0)\n",
    "df_data_text = pd.read_csv(\"dataset/text_data.csv\")\n",
    "sector2id = pickle.load(open(\"dataset/sector2id_dict.pkl\", \"rb\"))\n",
    "industry2id = pickle.load(open(\"dataset/industry2id_dict.pkl\",\"rb\"))\n",
    "id2sector = dict(zip(list(sector2id.values()), list(sector2id.keys())))\n",
    "id2industry = dict(zip(list(industry2id.values()), list(industry2id.keys())))\n",
    "revised_month_stock_df_dict = pickle.load(open(\"revised_month_stock_df_dict.pkl\", \"rb\"))\n",
    "sec_code2companyname = dict(zip(df_data[\"ticker\"], df_data[\"company_nm\"] ))\n",
    "use_sec_code_list_rev = np.sort(df_data[\"ticker\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_text_data = [np.array(text_id[1:-1].split(\", \")).astype(np.int32) for text_id in df_data_text[\"text_id\"]]\n",
    "use_label_data = [sector2id[label] for label in df_data[\"sector17\"]]\n",
    "use_industry_data = [industry2id[label] for label in df_data[\"sector33\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "seed = 0\n",
    "np.random.seed(seed)\n",
    "random_perm_all = np.random.permutation(len(use_text_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Stock Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_sec_code_list_rev = np.sort(df_data[\"ticker\"])\n",
    "similarity_mat_df = pd.read_csv(\"stock_data/all_similarity_mat.csv\",  index_col=0)\n",
    "similarity_mat_df = similarity_mat_df.T[df_data[\"ticker\"]].T[[str(val) for val in df_data[\"ticker\"]]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_num = max(sector2id.values())+ 1\n",
    "industry_num = max(industry2id.values())+ 1\n",
    "similarity_inner_mat = np.array(similarity_mat_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def out_actual_predicted_list(sentence_representation_all, theme_word_list, pooled_output, use_sec_code_list_eval, th_val = 0):\n",
    "    actual_list = []\n",
    "    predicted_list = []\n",
    "    for word, theme_vector_gpu in zip(theme_word_list, pooled_output):\n",
    "        theme_vector = theme_vector_gpu.detach().to(\"cpu\").numpy()\n",
    "        theme_code_set = set(pd.read_csv(\"./dataset/stock_themes/\" + word + \".tsv\", sep = \"\\t\", index_col = 0)[\"コード\"])\n",
    "        if np.mean(\n",
    "        [(sec_code in theme_code_set) for sec_code in use_sec_code_list_eval]) <= th_val:\n",
    "            continue\n",
    "        #print (word)\n",
    "        doc2theme_similarity_mat = sklearn.metrics.pairwise.cosine_similarity(sentence_representation_all, theme_vector.reshape(1, -1))\n",
    "        actual = np.array(range(len(use_sec_code_list_eval)))[[(sec_code in theme_code_set) for sec_code in use_sec_code_list_eval]]\n",
    "        predicted = np.argsort(doc2theme_similarity_mat[:,0])[-1::-1]\n",
    "        actual_list.append(list(actual))\n",
    "        predicted_list.append(list(predicted))\n",
    "    return actual_list, predicted_list "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_vectors(folda_name, use_ticker_list_train, sentence_representation_all_train, \n",
    "                use_ticker_list_valid, sentence_representation_all_valid, \n",
    "                use_ticker_list_test, sentence_representation_all_test):\n",
    "    with open(folda_name + \"/train_ticker_vectors.txt\", \"w\") as f:\n",
    "        for ticker, vector in zip(use_ticker_list_train, sentence_representation_all_train):\n",
    "            f.write(\"\\t\".join([str(ticker)] + [str(num) for num in vector]) + \"\\n\")\n",
    "    \n",
    "    with open(folda_name + \"/valid_ticker_vectors.txt\", \"w\") as f:\n",
    "        for ticker, vector in zip(use_ticker_list_valid, sentence_representation_all_valid):\n",
    "            f.write(\"\\t\".join([str(ticker)] + [str(num) for num in vector]) + \"\\n\")\n",
    "    \n",
    "    with open(folda_name + \"/test_ticker_vectors.txt\", \"w\") as f:\n",
    "        for ticker, vector in zip(use_ticker_list_test, sentence_representation_all_test):\n",
    "            f.write(\"\\t\".join([str(ticker)] + [str(num) for num in vector]) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_x = np.array(use_text_data)[random_perm_all][0:train_data_size]\n",
    "train_data_y = np.array(use_label_data)[random_perm_all][0:train_data_size]\n",
    "train_data_industry = np.array(use_industry_data)[random_perm_all][0:train_data_size]\n",
    "train_inner_stock_mat = similarity_inner_mat[random_perm_all].T[random_perm_all][:train_data_size, :train_data_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_data_x = np.array(use_text_data)[random_perm_all][train_data_size: train_data_size+ valid_size]\n",
    "valid_data_y = np.array(use_label_data)[random_perm_all][train_data_size: train_data_size+ valid_size]\n",
    "valid_data_industry = np.array(use_industry_data)[random_perm_all][train_data_size: train_data_size+ valid_size]\n",
    "valid_inner_stock_mat = similarity_inner_mat[random_perm_all].T[random_perm_all][\n",
    "    train_data_size: train_data_size+ valid_size, train_data_size: train_data_size+ valid_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_x = np.array(use_text_data)[random_perm_all][-test_size:]\n",
    "test_data_y = np.array(use_label_data)[random_perm_all][-test_size:]\n",
    "test_data_industry = np.array(use_industry_data)[random_perm_all][-test_size:]\n",
    "test_inner_stock_mat = similarity_inner_mat[random_perm_all].T[random_perm_all][-test_size:, -test_size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_train_examples = len(train_data_x)\n",
    "test_data_size = len(test_data_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_file_name = \"models/save\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all labels\n",
    "model_file_name = './models/BertModel_Mean/Sector_Stock_Name_1/001/pytorch_model.bin' \n",
    "model_file_name = './models/BertModel_Mean/Sector_Name_1/pytorch_model.bin'\n",
    "model_file_name = './models/BertModel_Mean/Sector_Stock_1/pytorch_model.bin'\n",
    "model_file_name = './models/BertModel_Mean/Sector_1/pytorch_model.bin'\n",
    "model_file_name = './models/BertModel_Mean/OnlyStock/pytorch_model.bin'\n",
    "\n",
    "# 2 labels\n",
    "model_file_name = './models/BertModel_Mean/Sector_Name_2labels/001/pytorch_model.bin'\n",
    "model_file_name = './models/BertModel_Mean/Stock_Sector_Name_2labels/001/pytorch_model.bin'\n",
    "# 5 labels\n",
    "model_file_name = './models/BertModel_Mean/Sector_Name_3/001/pytorch_model.bin''\n",
    "model_file_name = './models/BertModel_Mean/Sector_Stock_Name_3/001/pytorch_model.bin'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:pytorch_transformers.modeling_utils:loading configuration file ./models/finetuned_lm/config.json\n",
      "INFO:pytorch_transformers.modeling_utils:Model config {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"finetuning_task\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_labels\": 2,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"pruned_heads\": {},\n",
      "  \"torchscript\": false,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 32005\n",
      "}\n",
      "\n",
      "INFO:pytorch_transformers.modeling_utils:loading configuration file ./models/PyTorchPretrainendModel/config.json\n",
      "INFO:pytorch_transformers.modeling_utils:Model config {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"finetuning_task\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_labels\": 2,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"torchscript\": false,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 32005\n",
      "}\n",
      "\n",
      "INFO:pytorch_transformers.modeling_utils:loading weights file ./models/PyTorchPretrainendModel/pytorch_model.bin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertForSequenceMeanVec(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(32005, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): BertLayerNorm()\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (pooler_dev): BertPooler(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       "  (pooler_stock): BertPooler(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       "  (att): Linear(in_features=768, out_features=1, bias=True)\n",
       "  (att_industry): Linear(in_features=768, out_features=1, bias=True)\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=18, bias=True)\n",
       "  (classifier_industry): Linear(in_features=768, out_features=34, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_config = BertConfig.from_pretrained('./models/finetuned_lm/')\n",
    "#model_state_dict = torch.load(write_file_name + '/pytorch_model.bin')\n",
    "model_state_dict = torch.load(model_file_name)\n",
    "model = BertForSequenceMeanVec(bert_config, label_num, industry_num, state_dict=model_state_dict)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available()  else \"cpu\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_gpu = 2\n",
    "model.to(device)\n",
    "if -1 != -1:\n",
    "    try:\n",
    "        from apex.parallel import DistributedDataParallel as DDP\n",
    "    except ImportError:\n",
    "        raise ImportError(\n",
    "            \"Please install apex from https://www.github.com/nvidia/apex to use distributed and fp16 training.\")\n",
    "    model = DDP(model)\n",
    "elif n_gpu > 1:\n",
    "    model = torch.nn.DataParallel(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fct = CrossEntropyLoss(ignore_index = -1)\n",
    "loss_mse = MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sector\n",
      "same:  0.4040666\n",
      "other:  0.2505557\n",
      "industry\n",
      "same:  0.45016152\n",
      "other:  0.25452477\n"
     ]
    }
   ],
   "source": [
    "sentence_representation_all_test = evaluate_model_with_cosine_similarity(\n",
    "    model, test_data_x, test_data_y, test_data_industry, test_data_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Map@K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc2soc_similarity_mat = sklearn.metrics.pairwise.cosine_similarity(sentence_representation_all_test )\n",
    "sort_values = np.argsort(doc2soc_similarity_mat, axis = 1)[:,-1::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17 sector\n",
      "5 : 0.4723583333333333\n",
      "10 : 0.40502214285714283\n",
      "50 : 0.3247384807553308\n",
      "33 sector\n",
      "5 : 0.39632333333333325\n",
      "10 : 0.33754092813051145\n",
      "50 : 0.27227105958480513\n"
     ]
    }
   ],
   "source": [
    "predicted_list = [list(item[1:]) for item in sort_values]\n",
    "actual_list_all= [list(np.array(range(len(test_data_y)))[np.array(test_data_y) == test_data_y[index]]) for index in range(len(test_data_y))]\n",
    "actual_list_rev = []\n",
    "for index, item in enumerate(actual_list_all):\n",
    "    actual_list_rev.append(list(np.array(item)[np.array(item) != index]))\n",
    "\n",
    "print (\"17 sector\")\n",
    "for top_n in [5,10, 50]:\n",
    "    print (top_n , \":\",  ml_metrics.mapk(actual_list_rev, predicted_list, top_n))\n",
    "    \n",
    "    \n",
    "predicted_list = [list(item[1:]) for item in sort_values]\n",
    "actual_list_all= [list(np.array(range(len(test_data_industry)))[np.array(test_data_industry) == test_data_industry[index]]) for index in range(len(test_data_industry))]\n",
    "actual_list_rev = []\n",
    "for index, item in enumerate(actual_list_all):\n",
    "    actual_list_rev.append(list(np.array(item)[np.array(item) != index]))\n",
    "\n",
    "print (\"33 sector\")\n",
    "for top_n in [5,10, 50]:\n",
    "    print (top_n , \":\",  ml_metrics.mapk(actual_list_rev, predicted_list, top_n))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation for Theme Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "cos = nn.CosineSimilarity(dim=2, eps=1e-6)\n",
    "use_sec_code_list_eval = np.array(use_sec_code_list_rev)[random_perm_all][-test_size:]\n",
    "theme_word_list = [word.replace(\".tsv\", \"\") for word in os.listdir(\"./dataset/stock_themes\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "theme_token_list = []\n",
    "for word in theme_word_list:\n",
    "    text = \"[CLS] \" + word +  \" [SEP]\"\n",
    "    tokenized_text = tokenizer.tokenize(text)\n",
    "    tokenized_id = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
    "    theme_token_list.append(tokenized_id)\n",
    "\n",
    "max_seq_length_company = 8\n",
    "input_ids_list = []\n",
    "for tokenized_id in theme_token_list:\n",
    "    input_array = np.zeros(max_seq_length_company, dtype=np.int)\n",
    "    input_array[:min(max_seq_length_company, len(tokenized_id))] = tokenized_id[:min(max_seq_length_company, len(tokenized_id))]\n",
    "    input_ids_list.append(input_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = torch.LongTensor(np.array(input_ids_list).astype(np.int32))\n",
    "label_logits, pooled_output = model(input_ids, labels= None,  \n",
    "                                                label_industry = None,\n",
    "                                                labels_stock =  None, stock_vol = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "StockTheme\n",
      "274\n",
      "5 : 0.17610097323600973\n",
      "10 : 0.16069969843844537\n",
      "50 : 0.14372232991205527\n"
     ]
    }
   ],
   "source": [
    "actual_list, predicted_list  = out_actual_predicted_list(sentence_representation_all_test, \n",
    "                                                             theme_word_list, pooled_output, use_sec_code_list_eval, 0.00)\n",
    "print (\"StockTheme\")\n",
    "print (len(actual_list))\n",
    "for k in [5,10,50]:\n",
    "        print (k, \":\", ml_metrics.mapk(actual_list, predicted_list, k))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
